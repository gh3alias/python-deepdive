{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines - Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work along with this notebook you'll need the included data file, `car_data.csv`.\n",
    "\n",
    "We are going to want to split the data into different files based on some criteria of our choosing.\n",
    "\n",
    "For example, we may want to create a file that contains all pink cars, another file that contains all Mercedes brands, and another that contains only blue cars of a specific model year, etc.\n",
    "\n",
    "Let's first write a generator to parse the data for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def data_reader(f_name):\n",
    "    f = open(f_name)\n",
    "    try:\n",
    "        dialect = csv.Sniffer().sniff(f.read(2000))\n",
    "        f.seek(0)\n",
    "        reader = csv.reader(f, dialect=dialect)\n",
    "        yield from reader\n",
    "    finally:\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in data_reader('car_data.csv'):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our indices, output headers and data converters for this file - basically these are our configuration parameters for this data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'car_data.csv'\n",
    "\n",
    "idx_make = 0\n",
    "idx_model = 1\n",
    "idx_year = 2\n",
    "idx_vin = 3\n",
    "idx_color = 4\n",
    "\n",
    "headers = ('make', 'model', 'year', 'vin', 'color')\n",
    "\n",
    "converters = (str, str, int, str, str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a generator that will return the parsed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_parser():\n",
    "    data = data_reader(input_file)\n",
    "    next(data)  # skip header row\n",
    "    for row in data:\n",
    "        parsed_row = [converter(item)\n",
    "                      for converter, item in zip(converters, row)]\n",
    "        yield parsed_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just make sure this is working properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_parser()\n",
    "for _ in range(5):\n",
    "    print(next(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also write our coroutine decorator that will auto prime coroutines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coroutine(fn):\n",
    "    def inner(*args, **kwargs):\n",
    "        g = fn(*args, **kwargs)\n",
    "        next(g)\n",
    "        return g\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to write a coroutine that will create and write data to a file. We'll need to pass the output file name to the coroutine, and the coroutine will assume that the data is being passed in as a list (basically whatever is coming back from `data_parser`). To make it easier, we'll also pass it the column headers so we can include that in the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@coroutine\n",
    "def save_data(f_name, headers):\n",
    "    with open(f_name, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(headers)\n",
    "        while True:\n",
    "            data_row = yield\n",
    "            writer.writerow(data_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to create a filter coroutine that will have the following parameters:\n",
    "* filter function (predicate)\n",
    "* next coroutine to send to in the pipeline\n",
    "\n",
    "That filter coroutine will receive a data row and test if the predicate applied to that data row is True. If it is, it will send the row to the next stage (target) of the pipeline, otherwise it just ignores the data row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@coroutine\n",
    "def filter_data(filter_predicate, target):\n",
    "    while True:\n",
    "        data_row = yield\n",
    "        if filter_predicate(data_row):\n",
    "            target.send(data_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's write our broadcaster. It just sends received data to all the generators specified in the `targets` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@coroutine\n",
    "def broadcast(targets):\n",
    "    while True:\n",
    "        data_row = yield\n",
    "        for target in targets:\n",
    "            target.send(data_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we're now ready to put all this together.\n",
    "\n",
    "```\n",
    "     data                      \n",
    "      |                      |--> filter --> save\n",
    "      v                      |\n",
    "process_data --> broadcast --|--> filter --> save\n",
    "                             |\n",
    "                             |--> filter --> save\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    data = data_parser()\n",
    "    \n",
    "    out_pink_cars = save_data('pink_cars.csv', headers)\n",
    "    out_ford_green = save_data('ford_green.csv', headers)\n",
    "    out_older = save_data('older.csv', headers)\n",
    "    \n",
    "    filter_pink_cars = filter_data(lambda d: d[idx_color].lower() == 'pink',\n",
    "                                   out_pink_cars)\n",
    "    \n",
    "    def pred_ford_green(data_row):\n",
    "        return (data_row[idx_make].lower() == 'ford' \n",
    "                and data_row[idx_color].lower() == 'green')\n",
    "    \n",
    "    filter_ford_green = filter_data(pred_ford_green, out_ford_green)\n",
    "    filter_older = filter_data(lambda d: d[idx_year] <= 2010, out_older)\n",
    "    filters = (filter_pink_cars, filter_ford_green, filter_older)\n",
    "    broadcaster = broadcast(filters)\n",
    "    \n",
    "    for row in data:\n",
    "        broadcaster.send(row)\n",
    "    \n",
    "    print('Finished processing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's call it and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what those files contain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_file_data():\n",
    "    for file_name in ('pink_cars.csv', 'ford_green.csv', 'older.csv'):\n",
    "        print(f'***** {file_name} *****')\n",
    "        for row in data_reader(file_name):\n",
    "            print(row)\n",
    "        print('\\n\\n\\n')\n",
    "\n",
    "print_file_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one more bit of cleanup I want to do though.\n",
    "\n",
    "I would prefer to have the definition of my pipeline not also be the consumer of the data. Just trying to keep functionality more separated.\n",
    "\n",
    "So let's rewrite change `process_data` to just be another step in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@coroutine\n",
    "def pipeline_coro():\n",
    "    out_pink_cars = save_data('pink_cars.csv', headers)\n",
    "    out_ford_green = save_data('ford_green.csv', headers)\n",
    "    out_older = save_data('older.csv', headers)\n",
    "    \n",
    "    filter_pink_cars = filter_data(lambda d: d[idx_color].lower() == 'pink',\n",
    "                                  out_pink_cars)\n",
    "    \n",
    "    def pred_ford_green(data_row):\n",
    "        return (data_row[idx_make].lower() == 'ford'\n",
    "               and data_row[idx_color].lower() == 'green')\n",
    "    filter_ford_green = filter_data(pred_ford_green, out_ford_green)\n",
    "    filter_older = filter_data(lambda d: d[idx_year] <= 2010, out_older)\n",
    "    \n",
    "    filters = (filter_pink_cars, filter_ford_green, filter_older)\n",
    "    \n",
    "    broadcaster = broadcast(filters)\n",
    "    \n",
    "    while True:\n",
    "        data_row = yield\n",
    "        broadcaster.send(data_row)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can use the pipeline this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline_coro()\n",
    "data = data_parser()\n",
    "for row in data:\n",
    "    pipe.send(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so now let's make sure the correct data is in those output files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_file_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh, we get an exception. Why did the parser fail to figure out the dialect of the file?\n",
    "\n",
    "Let's see what's in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pink_cars.csv') as f:\n",
    "    for row in f:\n",
    "        print('row', row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is empty!!\n",
    "\n",
    "The issue is that our files have not been closed yet!\n",
    "\n",
    "The pipeline coroutine is still active, so nothing go released or closed - including the endpoints of our pipeline.\n",
    "\n",
    "Fortunately this is easy to do - we just need to close the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we should be able to read those files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_file_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, so just to recap, here's how we would use our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline_coro()\n",
    "data = data_parser()\n",
    "for row in data:\n",
    "    pipe.send(row)\n",
    "pipe.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... Notice how we open the pipeline, and then close it?\n",
    "Does this remind you of a context manager?\n",
    "\n",
    "Let's write a context manager for our pipeline - that way we'll never forget to close it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def pipeline():\n",
    "    p = pipeline_coro()\n",
    "    try:\n",
    "        yield p\n",
    "    finally:\n",
    "        p.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can use it this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pipeline() as pipe:\n",
    "    data = data_parser()\n",
    "    for row in data:\n",
    "        pipe.send(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, let's just make sure the files are OK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_file_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
